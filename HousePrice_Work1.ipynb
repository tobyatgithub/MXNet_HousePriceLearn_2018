{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dirctory = os.getcwd()\n",
    "\n",
    "train = pd.read_csv(str(dirctory)+\"/kaggle_house_pred_train.csv\")\n",
    "test = pd.read_csv(str(dirctory)+\"/kaggle_house_pred_test.csv\")\n",
    "all_X = pd.concat((train.loc[:, 'MSSubClass':'SaleCondition'],\n",
    "                      test.loc[:, 'MSSubClass':'SaleCondition']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 81)\n",
      "(1459, 80)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities    ...     PoolArea PoolQC Fence MiscFeature MiscVal  \\\n",
       "0         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "1         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "2         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "3         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "4         Lvl    AllPub    ...            0    NaN   NaN         NaN       0   \n",
       "\n",
       "  MoSold YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0      2   2008        WD         Normal     208500  \n",
       "1      5   2007        WD         Normal     181500  \n",
       "2      9   2008        WD         Normal     223500  \n",
       "3      2   2006        WD        Abnorml     140000  \n",
       "4     12   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond',\n",
      "       'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2',\n",
      "       'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n",
      "       'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath',\n",
      "       'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces',\n",
      "       'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF',\n",
      "       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal',\n",
      "       'MoSold', 'YrSold'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "numeric_feats = all_X.dtypes[all_X.dtypes != \"object\"].index\n",
    "print(numeric_feats)\n",
    "all_X[numeric_feats] = all_X[numeric_feats].apply(lambda x: (x - x.mean())/(x.std()))  #normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_X = pd.get_dummies(all_X, dummy_na=True) #turn category values into numerical dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_X = all_X.fillna(all_X.mean()) #intersting...this seems to fill each column with local mean!\n",
    "#print(all_X.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_train = train.shape[0]\n",
    "\n",
    "X_train = all_X[:num_train].as_matrix()\n",
    "X_test = all_X[num_train:].as_matrix()\n",
    "y_train = train.SalePrice.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet import ndarray as nd\n",
    "from mxnet import autograd\n",
    "from mxnet import gluon\n",
    "\n",
    "X_train = nd.array(X_train)\n",
    "y_train = nd.array(y_train)\n",
    "y_train.reshape((num_train, 1))\n",
    "\n",
    "X_test = nd.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define loss function\n",
    "square_loss = gluon.loss.L2Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_rmse_log(net, X_train, y_train):\n",
    "    num_train = X_train.shape[0]\n",
    "    clipped_preds = nd.clip(net(X_train), 1, float('inf'))\n",
    "    return np.sqrt(2 * nd.sum(square_loss(nd.log(clipped_preds), nd.log(y_train))).asscalar() / num_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_net():\n",
    "    net = gluon.nn.Sequential()\n",
    "    with net.name_scope():\n",
    "        net.add(gluon.nn.Dense(1))\n",
    "    net.initialize()\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi']= 120\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train(net, X_train, y_train, X_test, y_test, epochs,\n",
    "          verbose_epoch, learning_rate, weight_decay):\n",
    "    train_loss = []\n",
    "    if X_test is not None:\n",
    "        test_loss = []\n",
    "    batch_size = 100\n",
    "    dataset_train = gluon.data.ArrayDataset(X_train, y_train)\n",
    "    data_iter_train = gluon.data.DataLoader(\n",
    "        dataset_train, batch_size,shuffle=True)\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'adam',\n",
    "                            {'learning_rate': learning_rate,\n",
    "                             'wd': weight_decay})\n",
    "    net.collect_params().initialize(force_reinit=True)\n",
    "    for epoch in range(epochs):\n",
    "        for data, label in data_iter_train:\n",
    "            with autograd.record():\n",
    "                output = net(data)\n",
    "                loss = square_loss(output, label)\n",
    "            loss.backward()\n",
    "            trainer.step(batch_size)\n",
    "\n",
    "            cur_train_loss = get_rmse_log(net, X_train, y_train)\n",
    "        #if epoch > verbose_epoch:\n",
    "            #print(\"Epoch %d, train loss: %f\" % (epoch, cur_train_loss))\n",
    "        #train_loss.append(cur_train_loss)\n",
    "        if X_test is not None:\n",
    "            cur_test_loss = get_rmse_log(net, X_test, y_test)\n",
    "            test_loss.append(cur_test_loss)\n",
    "    plt.plot(train_loss)\n",
    "    plt.legend(['train'])\n",
    "    if X_test is not None:\n",
    "        plt.plot(test_loss)\n",
    "        plt.legend(['train','test'])\n",
    "    #plt.show()\n",
    "    if X_test is not None:\n",
    "        return cur_train_loss, cur_test_loss\n",
    "    else:\n",
    "        return cur_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def k_fold_cross_valid(k, epochs, verbose_epoch, X_train, y_train,\n",
    "                       learning_rate, weight_decay):\n",
    "    assert k > 1\n",
    "    fold_size = X_train.shape[0] // k\n",
    "    train_loss_sum = 0.0\n",
    "    test_loss_sum = 0.0\n",
    "    for test_i in range(k):\n",
    "        X_val_test = X_train[test_i * fold_size: (test_i + 1) * fold_size, :]\n",
    "        y_val_test = y_train[test_i * fold_size: (test_i + 1) * fold_size]\n",
    "\n",
    "        val_train_defined = False\n",
    "        for i in range(k):\n",
    "            if i != test_i:\n",
    "                X_cur_fold = X_train[i * fold_size: (i + 1) * fold_size, :]\n",
    "                y_cur_fold = y_train[i * fold_size: (i + 1) * fold_size]\n",
    "                if not val_train_defined:\n",
    "                    X_val_train = X_cur_fold\n",
    "                    y_val_train = y_cur_fold\n",
    "                    val_train_defined = True\n",
    "                else:\n",
    "                    X_val_train = nd.concat(X_val_train, X_cur_fold, dim=0)\n",
    "                    y_val_train = nd.concat(y_val_train, y_cur_fold, dim=0)\n",
    "        net = get_net()\n",
    "        train_loss, test_loss = train(\n",
    "            net, X_val_train, y_val_train, X_val_test, y_val_test,\n",
    "            epochs, verbose_epoch, learning_rate, weight_decay)\n",
    "        train_loss_sum += train_loss\n",
    "        #print(\"Test loss: %f\" % test_loss)\n",
    "        test_loss_sum += test_loss\n",
    "    return train_loss_sum / k, test_loss_sum / k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>SaleType_Oth</th>\n",
       "      <th>SaleType_WD</th>\n",
       "      <th>SaleType_nan</th>\n",
       "      <th>SaleCondition_Abnorml</th>\n",
       "      <th>SaleCondition_AdjLand</th>\n",
       "      <th>SaleCondition_Alloca</th>\n",
       "      <th>SaleCondition_Family</th>\n",
       "      <th>SaleCondition_Normal</th>\n",
       "      <th>SaleCondition_Partial</th>\n",
       "      <th>SaleCondition_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.067320</td>\n",
       "      <td>-0.184443</td>\n",
       "      <td>-0.217841</td>\n",
       "      <td>0.646073</td>\n",
       "      <td>-0.507197</td>\n",
       "      <td>1.046078</td>\n",
       "      <td>0.896679</td>\n",
       "      <td>0.523038</td>\n",
       "      <td>0.580708</td>\n",
       "      <td>-0.29303</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.873466</td>\n",
       "      <td>0.458096</td>\n",
       "      <td>-0.072032</td>\n",
       "      <td>-0.063174</td>\n",
       "      <td>2.187904</td>\n",
       "      <td>0.154737</td>\n",
       "      <td>-0.395536</td>\n",
       "      <td>-0.569893</td>\n",
       "      <td>1.177709</td>\n",
       "      <td>-0.29303</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.067320</td>\n",
       "      <td>-0.055935</td>\n",
       "      <td>0.137173</td>\n",
       "      <td>0.646073</td>\n",
       "      <td>-0.507197</td>\n",
       "      <td>0.980053</td>\n",
       "      <td>0.848819</td>\n",
       "      <td>0.333448</td>\n",
       "      <td>0.097840</td>\n",
       "      <td>-0.29303</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.302516</td>\n",
       "      <td>-0.398622</td>\n",
       "      <td>-0.078371</td>\n",
       "      <td>0.646073</td>\n",
       "      <td>-0.507197</td>\n",
       "      <td>-1.859033</td>\n",
       "      <td>-0.682695</td>\n",
       "      <td>-0.569893</td>\n",
       "      <td>-0.494771</td>\n",
       "      <td>-0.29303</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.067320</td>\n",
       "      <td>0.629439</td>\n",
       "      <td>0.518814</td>\n",
       "      <td>1.355319</td>\n",
       "      <td>-0.507197</td>\n",
       "      <td>0.947040</td>\n",
       "      <td>0.753100</td>\n",
       "      <td>1.381770</td>\n",
       "      <td>0.468770</td>\n",
       "      <td>-0.29303</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 331 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass  LotFrontage   LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0    0.067320    -0.184443 -0.217841     0.646073    -0.507197   1.046078   \n",
       "1   -0.873466     0.458096 -0.072032    -0.063174     2.187904   0.154737   \n",
       "2    0.067320    -0.055935  0.137173     0.646073    -0.507197   0.980053   \n",
       "3    0.302516    -0.398622 -0.078371     0.646073    -0.507197  -1.859033   \n",
       "4    0.067320     0.629439  0.518814     1.355319    -0.507197   0.947040   \n",
       "\n",
       "   YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2        ...          \\\n",
       "0      0.896679    0.523038    0.580708    -0.29303        ...           \n",
       "1     -0.395536   -0.569893    1.177709    -0.29303        ...           \n",
       "2      0.848819    0.333448    0.097840    -0.29303        ...           \n",
       "3     -0.682695   -0.569893   -0.494771    -0.29303        ...           \n",
       "4      0.753100    1.381770    0.468770    -0.29303        ...           \n",
       "\n",
       "   SaleType_Oth  SaleType_WD  SaleType_nan  SaleCondition_Abnorml  \\\n",
       "0             0            1             0                      0   \n",
       "1             0            1             0                      0   \n",
       "2             0            1             0                      0   \n",
       "3             0            1             0                      1   \n",
       "4             0            1             0                      0   \n",
       "\n",
       "   SaleCondition_AdjLand  SaleCondition_Alloca  SaleCondition_Family  \\\n",
       "0                      0                     0                     0   \n",
       "1                      0                     0                     0   \n",
       "2                      0                     0                     0   \n",
       "3                      0                     0                     0   \n",
       "4                      0                     0                     0   \n",
       "\n",
       "   SaleCondition_Normal  SaleCondition_Partial  SaleCondition_nan  \n",
       "0                     1                      0                  0  \n",
       "1                     1                      0                  0  \n",
       "2                     1                      0                  0  \n",
       "3                     0                      0                  0  \n",
       "4                     1                      0                  0  \n",
       "\n",
       "[5 rows x 331 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold validation: Avg train loss: 0.157907, Avg test loss: 0.166307\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAm8AAAGjCAYAAAB69PLaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1181a2550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "k = 5\n",
    "epochs = 200\n",
    "verbose_epoch = 95\n",
    "learning_rate = 5\n",
    "weight_decay = 0.0\n",
    "\n",
    "\n",
    "train_loss, test_loss = k_fold_cross_valid(k, epochs, verbose_epoch, X_train,\n",
    "                                           y_train, learning_rate, weight_decay)\n",
    "print(\"%d-fold validation: Avg train loss: %f, Avg test loss: %f\" %\n",
    "      (k, train_loss, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paramters: k = 4, epochs = 150, verbose_epoch = 90, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.238118, Avg test loss: 0.241871 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 90, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.242017, Avg test loss: 0.245786 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 90, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.238088, Avg test loss: 0.241790 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 90, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.519574, Avg test loss: 0.521705 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 90, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.246167, Avg test loss: 0.249961 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 90, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.176829, Avg test loss: 0.180885 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 90, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.181587, Avg test loss: 0.185567 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 90, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.176960, Avg test loss: 0.181042 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 90, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.462097, Avg test loss: 0.464094 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 90, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.187728, Avg test loss: 0.191507 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 90, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.165663, Avg test loss: 0.170821 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 90, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.175378, Avg test loss: 0.181062 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 90, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.165518, Avg test loss: 0.170587 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 90, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.440585, Avg test loss: 0.442514 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 90, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.283079, Avg test loss: 0.274566 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 91, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.238086, Avg test loss: 0.241708 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 91, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.242037, Avg test loss: 0.245907 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 91, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.238096, Avg test loss: 0.241814 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 91, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.519620, Avg test loss: 0.521751 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 91, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.246161, Avg test loss: 0.249943 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 91, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.176944, Avg test loss: 0.181008 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 91, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.181515, Avg test loss: 0.185551 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 91, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.176937, Avg test loss: 0.180987 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 91, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.462140, Avg test loss: 0.464140 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 91, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.187680, Avg test loss: 0.191468 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 91, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.165716, Avg test loss: 0.170590 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 91, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.175535, Avg test loss: 0.180834 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 91, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.165617, Avg test loss: 0.170675 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 91, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.440669, Avg test loss: 0.442620 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 91, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.283384, Avg test loss: 0.274574 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 92, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.238123, Avg test loss: 0.241897 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 92, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.242063, Avg test loss: 0.245830 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 92, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.238058, Avg test loss: 0.241853 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 92, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.519603, Avg test loss: 0.521732 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 92, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.246132, Avg test loss: 0.249976 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 92, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.176968, Avg test loss: 0.180957 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 92, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.181685, Avg test loss: 0.185778 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 92, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.176975, Avg test loss: 0.180978 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 92, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.462157, Avg test loss: 0.464177 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 92, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.187641, Avg test loss: 0.191800 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 92, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.165643, Avg test loss: 0.170665 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 92, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.175695, Avg test loss: 0.181930 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 92, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.165630, Avg test loss: 0.170621 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 92, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.440668, Avg test loss: 0.442601 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 92, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.280484, Avg test loss: 0.274487 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 93, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.238113, Avg test loss: 0.241852 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 93, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.241916, Avg test loss: 0.245761 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 93, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.238061, Avg test loss: 0.241844 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 93, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.519586, Avg test loss: 0.521738 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 93, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.246126, Avg test loss: 0.249966 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 93, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.177059, Avg test loss: 0.181060 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 93, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.181505, Avg test loss: 0.185595 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paramters: k = 4, epochs = 150, verbose_epoch = 93, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.177017, Avg test loss: 0.180964 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 93, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.462064, Avg test loss: 0.464065 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 93, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.187516, Avg test loss: 0.191499 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 93, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.165701, Avg test loss: 0.170857 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 93, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.175393, Avg test loss: 0.181264 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 93, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.165653, Avg test loss: 0.170702 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 93, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.440603, Avg test loss: 0.442551 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 93, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.280440, Avg test loss: 0.274586 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 94, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.238049, Avg test loss: 0.241748 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 94, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.242037, Avg test loss: 0.245821 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 94, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.238189, Avg test loss: 0.241890 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 94, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.519627, Avg test loss: 0.521776 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 94, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.246176, Avg test loss: 0.250068 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 94, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.176979, Avg test loss: 0.180975 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 94, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.181482, Avg test loss: 0.185660 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 94, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.177006, Avg test loss: 0.180997 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 94, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.462107, Avg test loss: 0.464123 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 94, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.187674, Avg test loss: 0.191814 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 94, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.165627, Avg test loss: 0.170716 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 94, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.175050, Avg test loss: 0.180840 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 94, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.165781, Avg test loss: 0.170895 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 94, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.440639, Avg test loss: 0.442579 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 94, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.291429, Avg test loss: 0.274548 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 95, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.238144, Avg test loss: 0.241956 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 95, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.242093, Avg test loss: 0.245862 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 95, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.238065, Avg test loss: 0.241741 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 95, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.519630, Avg test loss: 0.521771 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 95, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.246147, Avg test loss: 0.250017 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 95, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.176962, Avg test loss: 0.180923 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 95, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.181609, Avg test loss: 0.185632 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 95, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.176935, Avg test loss: 0.180906 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 95, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.462064, Avg test loss: 0.464084 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 95, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.187637, Avg test loss: 0.191774 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 95, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.165646, Avg test loss: 0.170786 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 95, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.175242, Avg test loss: 0.181239 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 95, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.165631, Avg test loss: 0.170621 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 95, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.440662, Avg test loss: 0.442592 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 95, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.279598, Avg test loss: 0.274541 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 96, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.238130, Avg test loss: 0.241882 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 96, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.242017, Avg test loss: 0.245770 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 96, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.238168, Avg test loss: 0.241942 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 96, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.519614, Avg test loss: 0.521767 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 96, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.246162, Avg test loss: 0.249981 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 96, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.176965, Avg test loss: 0.180999 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 96, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.181604, Avg test loss: 0.185543 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 96, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.176979, Avg test loss: 0.181007 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 96, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.462060, Avg test loss: 0.464063 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 96, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.187504, Avg test loss: 0.191444 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 96, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.165614, Avg test loss: 0.170735 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 96, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.175341, Avg test loss: 0.181336 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 96, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.165625, Avg test loss: 0.170676 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 96, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.440733, Avg test loss: 0.442671 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paramters: k = 4, epochs = 150, verbose_epoch = 96, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.282572, Avg test loss: 0.274547 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 97, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.238121, Avg test loss: 0.241834 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 97, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.242018, Avg test loss: 0.245804 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 97, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.238101, Avg test loss: 0.241749 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 97, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.519690, Avg test loss: 0.521836 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 97, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.246198, Avg test loss: 0.250090 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 97, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.176983, Avg test loss: 0.180935 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 97, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.181537, Avg test loss: 0.185605 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 97, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.177056, Avg test loss: 0.181096 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 97, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.462035, Avg test loss: 0.464048 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 97, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.187626, Avg test loss: 0.191660 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 97, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.165520, Avg test loss: 0.170611 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 97, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.175293, Avg test loss: 0.180845 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 97, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.165635, Avg test loss: 0.170663 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 97, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.440645, Avg test loss: 0.442582 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 97, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.279919, Avg test loss: 0.274561 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 98, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.238150, Avg test loss: 0.241958 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 98, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.241992, Avg test loss: 0.245821 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 98, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.238061, Avg test loss: 0.241723 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 98, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.519626, Avg test loss: 0.521764 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 98, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.246189, Avg test loss: 0.249936 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 98, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.176958, Avg test loss: 0.181028 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 98, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.181556, Avg test loss: 0.185732 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 98, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.176901, Avg test loss: 0.181001 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 98, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.462049, Avg test loss: 0.464061 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 98, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.187645, Avg test loss: 0.191599 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 98, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.165621, Avg test loss: 0.170617 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 98, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.175352, Avg test loss: 0.181025 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 98, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.165638, Avg test loss: 0.170684 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 98, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.440714, Avg test loss: 0.442652 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 98, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.278493, Avg test loss: 0.274705 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 99, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.238120, Avg test loss: 0.241814 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 99, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.241980, Avg test loss: 0.245807 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 99, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.238142, Avg test loss: 0.241861 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 99, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.519693, Avg test loss: 0.521824 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 99, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.246134, Avg test loss: 0.249892 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 99, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.176931, Avg test loss: 0.181047 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 99, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.181565, Avg test loss: 0.185493 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 99, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.176969, Avg test loss: 0.180988 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 99, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.462043, Avg test loss: 0.464043 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 99, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.187483, Avg test loss: 0.191654 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 99, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.165585, Avg test loss: 0.170619 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 99, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.175195, Avg test loss: 0.180810 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 99, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.165639, Avg test loss: 0.170564 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 99, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.440566, Avg test loss: 0.442502 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 99, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.280103, Avg test loss: 0.274551 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 100, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.238038, Avg test loss: 0.241793 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 100, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.241977, Avg test loss: 0.245847 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 100, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.238131, Avg test loss: 0.241864 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 100, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.519680, Avg test loss: 0.521833 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 100, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.246228, Avg test loss: 0.250034 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 100, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.177022, Avg test loss: 0.181034 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paramters: k = 4, epochs = 150, verbose_epoch = 100, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.181527, Avg test loss: 0.185560 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 100, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.177037, Avg test loss: 0.181033 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 100, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.462073, Avg test loss: 0.464085 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 100, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.187536, Avg test loss: 0.191740 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 100, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.165661, Avg test loss: 0.170720 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 100, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.175200, Avg test loss: 0.181293 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 100, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.165604, Avg test loss: 0.170532 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 100, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.440566, Avg test loss: 0.442511 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 100, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.283665, Avg test loss: 0.274548 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 101, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.238096, Avg test loss: 0.241812 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 101, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.242060, Avg test loss: 0.245901 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 101, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.238060, Avg test loss: 0.241767 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 101, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.519645, Avg test loss: 0.521793 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 101, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.246082, Avg test loss: 0.249946 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 101, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.176963, Avg test loss: 0.180980 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 101, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.181640, Avg test loss: 0.185661 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 101, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.177057, Avg test loss: 0.181044 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 101, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.462007, Avg test loss: 0.464025 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 101, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.187589, Avg test loss: 0.191597 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 101, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.165688, Avg test loss: 0.170706 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 101, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.175070, Avg test loss: 0.181094 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 101, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.165578, Avg test loss: 0.170551 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 101, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.440647, Avg test loss: 0.442574 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 101, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.279604, Avg test loss: 0.274575 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 102, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.238040, Avg test loss: 0.241764 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 102, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.242052, Avg test loss: 0.245800 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 102, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.238096, Avg test loss: 0.241863 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 102, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.519577, Avg test loss: 0.521727 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 102, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.246177, Avg test loss: 0.249980 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 102, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.176980, Avg test loss: 0.181025 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 102, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.181547, Avg test loss: 0.185686 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 102, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.176973, Avg test loss: 0.181008 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 102, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.462107, Avg test loss: 0.464108 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 102, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.187676, Avg test loss: 0.191737 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 102, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.165630, Avg test loss: 0.170756 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 102, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.175305, Avg test loss: 0.181059 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 102, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.165691, Avg test loss: 0.170794 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 102, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.440650, Avg test loss: 0.442581 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 102, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.280669, Avg test loss: 0.274451 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 103, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.238088, Avg test loss: 0.241903 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 103, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.241992, Avg test loss: 0.245675 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 103, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.238049, Avg test loss: 0.241814 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 103, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.519594, Avg test loss: 0.521742 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 103, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.246222, Avg test loss: 0.250009 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 103, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.176972, Avg test loss: 0.181009 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 103, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.181545, Avg test loss: 0.185522 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 103, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.176924, Avg test loss: 0.180965 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 103, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.462081, Avg test loss: 0.464095 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 103, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.187528, Avg test loss: 0.191766 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 103, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.165659, Avg test loss: 0.170653 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 103, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.175245, Avg test loss: 0.181733 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 103, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.165604, Avg test loss: 0.170662 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paramters: k = 4, epochs = 150, verbose_epoch = 103, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.440658, Avg test loss: 0.442592 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 103, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.279356, Avg test loss: 0.274597 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 104, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.238095, Avg test loss: 0.241790 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 104, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.242042, Avg test loss: 0.245766 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 104, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.238170, Avg test loss: 0.241841 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 104, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.519630, Avg test loss: 0.521762 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 104, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.246126, Avg test loss: 0.249956 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 104, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.177033, Avg test loss: 0.181058 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 104, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.181627, Avg test loss: 0.185708 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 104, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.177039, Avg test loss: 0.181051 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 104, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.461981, Avg test loss: 0.463984 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 104, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.187625, Avg test loss: 0.191604 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 104, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.165691, Avg test loss: 0.170705 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 104, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.174992, Avg test loss: 0.180995 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 104, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.165573, Avg test loss: 0.170613 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 104, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.440694, Avg test loss: 0.442633 \n",
      "\n",
      "paramters: k = 4, epochs = 150, verbose_epoch = 104, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.280788, Avg test loss: 0.274427 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 90, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.174746, Avg test loss: 0.178694 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 90, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.179329, Avg test loss: 0.183469 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 90, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.174728, Avg test loss: 0.178784 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 90, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.458375, Avg test loss: 0.460368 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 90, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.185794, Avg test loss: 0.189928 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 90, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.164026, Avg test loss: 0.169949 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 90, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.179319, Avg test loss: 0.198307 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 90, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.163998, Avg test loss: 0.169784 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 90, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.435355, Avg test loss: 0.437281 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 90, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314580, Avg test loss: 0.274280 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 90, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160213, Avg test loss: 0.168978 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 90, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.272862, Avg test loss: 0.269961 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 90, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160167, Avg test loss: 0.168811 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 90, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.429276, Avg test loss: 0.431180 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 90, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314675, Avg test loss: 0.275227 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 91, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.174802, Avg test loss: 0.178839 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 91, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.179354, Avg test loss: 0.183399 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 91, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.174774, Avg test loss: 0.178891 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 91, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.458304, Avg test loss: 0.460305 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 91, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.185758, Avg test loss: 0.189797 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 91, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.164061, Avg test loss: 0.169985 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 91, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.180013, Avg test loss: 0.196566 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 91, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.164019, Avg test loss: 0.169849 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 91, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.435247, Avg test loss: 0.437167 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 91, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314549, Avg test loss: 0.274260 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 91, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160171, Avg test loss: 0.168862 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 91, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.273055, Avg test loss: 0.269947 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 91, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160317, Avg test loss: 0.169091 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 91, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.429261, Avg test loss: 0.431153 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 91, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314663, Avg test loss: 0.275374 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 92, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.174785, Avg test loss: 0.178764 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 92, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.179304, Avg test loss: 0.183411 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 92, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.174770, Avg test loss: 0.178731 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 92, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.458324, Avg test loss: 0.460318 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 92, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.185798, Avg test loss: 0.189721 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paramters: k = 4, epochs = 200, verbose_epoch = 92, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.164087, Avg test loss: 0.169922 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 92, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.180000, Avg test loss: 0.192867 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 92, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.164044, Avg test loss: 0.169972 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 92, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.435280, Avg test loss: 0.437187 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 92, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314597, Avg test loss: 0.274280 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 92, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160260, Avg test loss: 0.169007 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 92, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.272582, Avg test loss: 0.269992 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 92, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160230, Avg test loss: 0.168742 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 92, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.429281, Avg test loss: 0.431178 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 92, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314695, Avg test loss: 0.275426 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 93, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.174776, Avg test loss: 0.178946 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 93, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.179403, Avg test loss: 0.183507 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 93, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.174735, Avg test loss: 0.178811 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 93, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.458379, Avg test loss: 0.460373 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 93, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.185821, Avg test loss: 0.189634 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 93, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.164030, Avg test loss: 0.169873 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 93, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.179414, Avg test loss: 0.195257 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 93, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.164032, Avg test loss: 0.169875 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 93, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.435335, Avg test loss: 0.437258 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 93, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314560, Avg test loss: 0.274205 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 93, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160184, Avg test loss: 0.168912 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 93, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.273910, Avg test loss: 0.269954 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 93, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160175, Avg test loss: 0.168685 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 93, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.429293, Avg test loss: 0.431198 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 93, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314671, Avg test loss: 0.275400 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 94, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.174738, Avg test loss: 0.178717 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 94, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.179354, Avg test loss: 0.183300 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 94, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.174736, Avg test loss: 0.178763 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 94, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.458382, Avg test loss: 0.460373 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 94, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.185749, Avg test loss: 0.189827 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 94, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.163973, Avg test loss: 0.169852 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 94, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.179723, Avg test loss: 0.194120 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 94, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.164007, Avg test loss: 0.169758 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 94, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.435340, Avg test loss: 0.437269 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 94, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314557, Avg test loss: 0.274194 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 94, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160111, Avg test loss: 0.168753 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 94, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.272415, Avg test loss: 0.269909 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 94, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160240, Avg test loss: 0.168870 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 94, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.429311, Avg test loss: 0.431192 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 94, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314678, Avg test loss: 0.275295 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 95, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.174717, Avg test loss: 0.178748 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 95, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.179394, Avg test loss: 0.183344 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 95, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.174735, Avg test loss: 0.178742 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 95, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.458372, Avg test loss: 0.460375 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 95, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.185721, Avg test loss: 0.189702 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 95, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.164044, Avg test loss: 0.170057 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 95, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.179840, Avg test loss: 0.196674 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 95, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.163999, Avg test loss: 0.169928 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 95, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.435337, Avg test loss: 0.437269 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 95, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314548, Avg test loss: 0.274286 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 95, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160260, Avg test loss: 0.168776 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 95, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.274908, Avg test loss: 0.269947 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paramters: k = 4, epochs = 200, verbose_epoch = 95, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160139, Avg test loss: 0.168833 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 95, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.429095, Avg test loss: 0.430996 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 95, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314689, Avg test loss: 0.275343 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 96, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.174778, Avg test loss: 0.178775 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 96, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.179319, Avg test loss: 0.183445 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 96, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.174668, Avg test loss: 0.178769 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 96, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.458380, Avg test loss: 0.460378 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 96, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.185672, Avg test loss: 0.189684 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 96, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.163955, Avg test loss: 0.169725 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 96, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.179742, Avg test loss: 0.196703 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 96, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.164007, Avg test loss: 0.169831 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 96, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.435308, Avg test loss: 0.437226 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 96, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314574, Avg test loss: 0.274201 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 96, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160165, Avg test loss: 0.168669 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 96, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.274206, Avg test loss: 0.269996 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 96, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160245, Avg test loss: 0.168709 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 96, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.429249, Avg test loss: 0.431141 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 96, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314676, Avg test loss: 0.275342 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 97, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.174775, Avg test loss: 0.178869 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 97, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.179318, Avg test loss: 0.183299 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 97, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.174769, Avg test loss: 0.178793 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 97, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.458269, Avg test loss: 0.460274 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 97, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.185681, Avg test loss: 0.189932 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 97, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.164055, Avg test loss: 0.169902 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 97, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.179641, Avg test loss: 0.193148 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 97, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.164078, Avg test loss: 0.169983 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 97, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.435384, Avg test loss: 0.437307 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 97, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314552, Avg test loss: 0.274129 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 97, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160255, Avg test loss: 0.168683 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 97, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.272114, Avg test loss: 0.269908 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 97, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160143, Avg test loss: 0.168956 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 97, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.429278, Avg test loss: 0.431165 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 97, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314725, Avg test loss: 0.275325 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 98, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.174807, Avg test loss: 0.178852 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 98, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.179338, Avg test loss: 0.183554 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 98, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.174753, Avg test loss: 0.178871 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 98, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.458342, Avg test loss: 0.460334 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 98, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.185661, Avg test loss: 0.189806 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 98, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.164034, Avg test loss: 0.169779 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 98, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.179795, Avg test loss: 0.192465 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 98, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.164005, Avg test loss: 0.169626 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 98, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.435354, Avg test loss: 0.437271 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 98, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314562, Avg test loss: 0.274291 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 98, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160248, Avg test loss: 0.168747 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 98, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.273662, Avg test loss: 0.269968 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 98, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160204, Avg test loss: 0.168755 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 98, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.429144, Avg test loss: 0.431036 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 98, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314686, Avg test loss: 0.275306 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 99, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.174739, Avg test loss: 0.178812 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 99, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.179280, Avg test loss: 0.183387 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 99, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.174773, Avg test loss: 0.178850 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 99, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.458410, Avg test loss: 0.460415 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paramters: k = 4, epochs = 200, verbose_epoch = 99, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.185650, Avg test loss: 0.189976 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 99, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.163980, Avg test loss: 0.169808 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 99, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.179855, Avg test loss: 0.196182 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 99, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.163987, Avg test loss: 0.169868 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 99, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.435392, Avg test loss: 0.437325 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 99, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314582, Avg test loss: 0.274225 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 99, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160322, Avg test loss: 0.168904 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 99, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.273361, Avg test loss: 0.269966 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 99, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160161, Avg test loss: 0.168846 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 99, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.429199, Avg test loss: 0.431097 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 99, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314693, Avg test loss: 0.275383 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 100, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.174788, Avg test loss: 0.178802 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 100, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.179308, Avg test loss: 0.183436 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 100, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.174760, Avg test loss: 0.178844 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 100, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.458358, Avg test loss: 0.460365 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 100, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.185718, Avg test loss: 0.189831 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 100, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.163979, Avg test loss: 0.169820 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 100, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.179859, Avg test loss: 0.193463 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 100, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.164035, Avg test loss: 0.169903 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 100, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.435299, Avg test loss: 0.437229 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 100, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314551, Avg test loss: 0.274229 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 100, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160341, Avg test loss: 0.168818 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 100, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.271480, Avg test loss: 0.269950 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 100, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160197, Avg test loss: 0.168718 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 100, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.429203, Avg test loss: 0.431087 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 100, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314704, Avg test loss: 0.275321 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 101, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.174710, Avg test loss: 0.178731 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 101, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.179323, Avg test loss: 0.183388 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 101, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.174732, Avg test loss: 0.178737 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 101, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.458385, Avg test loss: 0.460384 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 101, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.185559, Avg test loss: 0.189555 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 101, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.164023, Avg test loss: 0.170083 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 101, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.180262, Avg test loss: 0.195860 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 101, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.163991, Avg test loss: 0.169759 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 101, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.435387, Avg test loss: 0.437311 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 101, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314564, Avg test loss: 0.274290 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 101, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160193, Avg test loss: 0.168707 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 101, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.273535, Avg test loss: 0.270006 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 101, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160250, Avg test loss: 0.168885 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 101, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.429066, Avg test loss: 0.430969 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 101, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314679, Avg test loss: 0.275316 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 102, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.174760, Avg test loss: 0.178833 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 102, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.179267, Avg test loss: 0.183533 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 102, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.174723, Avg test loss: 0.178774 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 102, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.458328, Avg test loss: 0.460333 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 102, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.185656, Avg test loss: 0.189721 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 102, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.164113, Avg test loss: 0.170086 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 102, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.179410, Avg test loss: 0.196080 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 102, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.164047, Avg test loss: 0.169856 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 102, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.435367, Avg test loss: 0.437289 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 102, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314584, Avg test loss: 0.274206 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 102, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160190, Avg test loss: 0.168818 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paramters: k = 4, epochs = 200, verbose_epoch = 102, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.272768, Avg test loss: 0.269920 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 102, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160200, Avg test loss: 0.168989 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 102, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.429301, Avg test loss: 0.431193 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 102, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314678, Avg test loss: 0.275331 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 103, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.174730, Avg test loss: 0.178792 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 103, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.179346, Avg test loss: 0.183361 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 103, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.174779, Avg test loss: 0.178799 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 103, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.458339, Avg test loss: 0.460338 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 103, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.185776, Avg test loss: 0.189738 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 103, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.164044, Avg test loss: 0.170036 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 103, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.180348, Avg test loss: 0.196292 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 103, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.163992, Avg test loss: 0.169831 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 103, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.435345, Avg test loss: 0.437283 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 103, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314552, Avg test loss: 0.274154 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 103, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160083, Avg test loss: 0.168775 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 103, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.274415, Avg test loss: 0.269904 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 103, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160206, Avg test loss: 0.168877 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 103, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.429226, Avg test loss: 0.431112 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 103, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314724, Avg test loss: 0.275404 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 104, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.174805, Avg test loss: 0.178784 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 104, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.179362, Avg test loss: 0.183362 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 104, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.174698, Avg test loss: 0.178719 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 104, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.458263, Avg test loss: 0.460256 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 104, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.185751, Avg test loss: 0.189870 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 104, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.164002, Avg test loss: 0.169776 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 104, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.179949, Avg test loss: 0.193865 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 104, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.164002, Avg test loss: 0.169956 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 104, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.435350, Avg test loss: 0.437270 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 104, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314572, Avg test loss: 0.274280 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 104, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160286, Avg test loss: 0.168912 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 104, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.272124, Avg test loss: 0.269888 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 104, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160208, Avg test loss: 0.168821 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 104, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.429321, Avg test loss: 0.431218 \n",
      "\n",
      "paramters: k = 4, epochs = 200, verbose_epoch = 104, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314725, Avg test loss: 0.275358 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 90, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.164357, Avg test loss: 0.169964 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 90, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.176258, Avg test loss: 0.184276 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 90, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.164333, Avg test loss: 0.169847 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 90, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.436546, Avg test loss: 0.438476 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 90, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314509, Avg test loss: 0.273958 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 90, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.159050, Avg test loss: 0.167893 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 90, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.272141, Avg test loss: 0.269554 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 90, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.158985, Avg test loss: 0.167709 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 90, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.428624, Avg test loss: 0.430511 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 90, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314424, Avg test loss: 0.275147 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 90, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151242, Avg test loss: 0.161452 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 90, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.270837, Avg test loss: 0.266967 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 90, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151295, Avg test loss: 0.161385 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 90, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.427365, Avg test loss: 0.429246 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 90, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.312571, Avg test loss: 0.274222 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 91, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.164392, Avg test loss: 0.169918 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 91, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.176151, Avg test loss: 0.184098 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 91, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.164284, Avg test loss: 0.169881 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paramters: k = 4, epochs = 250, verbose_epoch = 91, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.436678, Avg test loss: 0.438591 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 91, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314492, Avg test loss: 0.274071 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 91, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.159082, Avg test loss: 0.167900 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 91, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.272210, Avg test loss: 0.269654 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 91, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.159051, Avg test loss: 0.167662 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 91, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.428585, Avg test loss: 0.430487 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 91, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314472, Avg test loss: 0.275297 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 91, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151204, Avg test loss: 0.161362 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 91, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.271020, Avg test loss: 0.266969 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 91, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151255, Avg test loss: 0.161393 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 91, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.427270, Avg test loss: 0.429163 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 91, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.312520, Avg test loss: 0.274245 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 92, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.164343, Avg test loss: 0.169922 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 92, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.176191, Avg test loss: 0.183488 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 92, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.164299, Avg test loss: 0.169754 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 92, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.436589, Avg test loss: 0.438518 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 92, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314444, Avg test loss: 0.273999 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 92, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.158969, Avg test loss: 0.167683 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 92, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.271227, Avg test loss: 0.269613 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 92, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.158999, Avg test loss: 0.167661 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 92, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.428666, Avg test loss: 0.430562 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 92, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314441, Avg test loss: 0.275271 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 92, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151252, Avg test loss: 0.161461 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 92, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.270029, Avg test loss: 0.266980 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 92, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151294, Avg test loss: 0.161288 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 92, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.427353, Avg test loss: 0.429242 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 92, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.312587, Avg test loss: 0.274158 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 93, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.164341, Avg test loss: 0.169867 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 93, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.176027, Avg test loss: 0.184138 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 93, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.164407, Avg test loss: 0.169937 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 93, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.436631, Avg test loss: 0.438557 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 93, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314488, Avg test loss: 0.274023 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 93, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.159012, Avg test loss: 0.167627 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 93, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.271776, Avg test loss: 0.269572 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 93, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.159007, Avg test loss: 0.167849 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 93, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.428514, Avg test loss: 0.430403 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 93, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314423, Avg test loss: 0.275163 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 93, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151268, Avg test loss: 0.161455 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 93, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.270880, Avg test loss: 0.266963 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 93, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151289, Avg test loss: 0.161435 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 93, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.427248, Avg test loss: 0.429123 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 93, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.312520, Avg test loss: 0.274162 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 94, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.164311, Avg test loss: 0.169758 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 94, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.175958, Avg test loss: 0.184471 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 94, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.164297, Avg test loss: 0.169616 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 94, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.436609, Avg test loss: 0.438530 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 94, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314501, Avg test loss: 0.273977 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 94, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.159090, Avg test loss: 0.168086 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 94, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.272819, Avg test loss: 0.269584 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 94, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.158994, Avg test loss: 0.167936 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 94, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.428700, Avg test loss: 0.430597 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 94, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314453, Avg test loss: 0.275242 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paramters: k = 4, epochs = 250, verbose_epoch = 94, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151268, Avg test loss: 0.161397 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 94, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.269715, Avg test loss: 0.267030 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 94, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151215, Avg test loss: 0.161306 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 94, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.427235, Avg test loss: 0.429136 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 94, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.312629, Avg test loss: 0.274320 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 95, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.164305, Avg test loss: 0.169894 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 95, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.175919, Avg test loss: 0.183987 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 95, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.164345, Avg test loss: 0.169823 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 95, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.436589, Avg test loss: 0.438518 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 95, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314458, Avg test loss: 0.274026 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 95, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.158997, Avg test loss: 0.167732 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 95, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.273599, Avg test loss: 0.269629 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 95, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.159056, Avg test loss: 0.168092 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 95, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.428656, Avg test loss: 0.430545 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 95, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314404, Avg test loss: 0.275162 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 95, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151251, Avg test loss: 0.161348 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 95, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.269995, Avg test loss: 0.266961 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 95, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151201, Avg test loss: 0.161281 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 95, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.427157, Avg test loss: 0.429045 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 95, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.312572, Avg test loss: 0.274143 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 96, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.164426, Avg test loss: 0.169822 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 96, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.176215, Avg test loss: 0.184872 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 96, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.164323, Avg test loss: 0.169710 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 96, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.436594, Avg test loss: 0.438513 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 96, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314445, Avg test loss: 0.273958 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 96, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.159057, Avg test loss: 0.167794 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 96, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.271916, Avg test loss: 0.269561 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 96, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.159042, Avg test loss: 0.167790 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 96, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.428549, Avg test loss: 0.430442 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 96, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314447, Avg test loss: 0.275267 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 96, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151100, Avg test loss: 0.161315 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 96, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.269767, Avg test loss: 0.266954 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 96, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151236, Avg test loss: 0.161330 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 96, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.427168, Avg test loss: 0.429041 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 96, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.312549, Avg test loss: 0.274141 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 97, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.164318, Avg test loss: 0.169738 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 97, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.176238, Avg test loss: 0.184243 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 97, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.164376, Avg test loss: 0.169658 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 97, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.436558, Avg test loss: 0.438495 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 97, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314520, Avg test loss: 0.274092 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 97, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.159085, Avg test loss: 0.167559 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 97, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.273297, Avg test loss: 0.269567 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 97, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.159057, Avg test loss: 0.167897 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 97, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.428759, Avg test loss: 0.430655 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 97, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314413, Avg test loss: 0.275156 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 97, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151217, Avg test loss: 0.161338 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 97, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.269383, Avg test loss: 0.266979 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 97, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151200, Avg test loss: 0.161356 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 97, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.427387, Avg test loss: 0.429271 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 97, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.312489, Avg test loss: 0.273980 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 98, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.164344, Avg test loss: 0.169835 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 98, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.176008, Avg test loss: 0.183952 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paramters: k = 4, epochs = 250, verbose_epoch = 98, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.164368, Avg test loss: 0.170041 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 98, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.436585, Avg test loss: 0.438515 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 98, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314486, Avg test loss: 0.274007 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 98, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.159051, Avg test loss: 0.167910 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 98, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.273099, Avg test loss: 0.269630 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 98, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.158970, Avg test loss: 0.167729 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 98, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.428671, Avg test loss: 0.430576 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 98, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314440, Avg test loss: 0.275126 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 98, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151231, Avg test loss: 0.161306 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 98, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.270420, Avg test loss: 0.266988 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 98, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151237, Avg test loss: 0.161348 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 98, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.427202, Avg test loss: 0.429078 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 98, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.312482, Avg test loss: 0.274034 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 99, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.164327, Avg test loss: 0.169891 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 99, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.176032, Avg test loss: 0.184371 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 99, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.164369, Avg test loss: 0.169776 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 99, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.436632, Avg test loss: 0.438561 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 99, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314482, Avg test loss: 0.274017 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 99, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.159007, Avg test loss: 0.167788 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 99, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.272753, Avg test loss: 0.269578 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 99, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.159049, Avg test loss: 0.167674 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 99, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.428690, Avg test loss: 0.430589 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 99, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314415, Avg test loss: 0.275162 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 99, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151198, Avg test loss: 0.161336 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 99, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.269911, Avg test loss: 0.266966 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 99, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151164, Avg test loss: 0.161296 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 99, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.427319, Avg test loss: 0.429221 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 99, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.312572, Avg test loss: 0.274205 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 100, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.164310, Avg test loss: 0.169753 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 100, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.176151, Avg test loss: 0.183265 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 100, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.164326, Avg test loss: 0.169847 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 100, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.436687, Avg test loss: 0.438603 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 100, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314485, Avg test loss: 0.274029 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 100, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.159116, Avg test loss: 0.167809 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 100, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.272054, Avg test loss: 0.269533 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 100, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.159054, Avg test loss: 0.167864 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 100, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.428604, Avg test loss: 0.430493 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 100, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314424, Avg test loss: 0.275177 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 100, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151210, Avg test loss: 0.161228 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 100, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.271173, Avg test loss: 0.266988 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 100, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151208, Avg test loss: 0.161392 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 100, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.427273, Avg test loss: 0.429154 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 100, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.312555, Avg test loss: 0.274075 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 101, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.164333, Avg test loss: 0.169902 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 101, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.176106, Avg test loss: 0.183556 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 101, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.164375, Avg test loss: 0.169864 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 101, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.436555, Avg test loss: 0.438488 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 101, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314453, Avg test loss: 0.273992 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 101, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.159005, Avg test loss: 0.167682 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 101, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.273081, Avg test loss: 0.269612 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 101, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.159025, Avg test loss: 0.167928 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 101, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.428655, Avg test loss: 0.430553 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paramters: k = 4, epochs = 250, verbose_epoch = 101, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314428, Avg test loss: 0.275198 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 101, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151302, Avg test loss: 0.161426 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 101, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.272681, Avg test loss: 0.266997 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 101, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151238, Avg test loss: 0.161363 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 101, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.427354, Avg test loss: 0.429239 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 101, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.312501, Avg test loss: 0.274084 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 102, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.164377, Avg test loss: 0.169853 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 102, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.176215, Avg test loss: 0.184447 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 102, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.164352, Avg test loss: 0.169859 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 102, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.436641, Avg test loss: 0.438574 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 102, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314473, Avg test loss: 0.274031 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 102, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.159038, Avg test loss: 0.167875 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 102, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.272228, Avg test loss: 0.269661 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 102, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.159101, Avg test loss: 0.167695 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 102, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.428690, Avg test loss: 0.430579 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 102, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314426, Avg test loss: 0.275174 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 102, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151208, Avg test loss: 0.161325 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 102, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.270931, Avg test loss: 0.267039 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 102, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151238, Avg test loss: 0.161252 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 102, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.427516, Avg test loss: 0.429397 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 102, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.312581, Avg test loss: 0.274275 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 103, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.164339, Avg test loss: 0.169853 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 103, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.176155, Avg test loss: 0.184635 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 103, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.164293, Avg test loss: 0.169778 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 103, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.436570, Avg test loss: 0.438499 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 103, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314477, Avg test loss: 0.274035 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 103, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.158993, Avg test loss: 0.167833 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 103, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.271782, Avg test loss: 0.269624 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 103, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.158958, Avg test loss: 0.167832 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 103, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.428636, Avg test loss: 0.430527 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 103, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314410, Avg test loss: 0.275144 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 103, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151216, Avg test loss: 0.161261 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 103, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.271413, Avg test loss: 0.266967 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 103, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151239, Avg test loss: 0.161443 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 103, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.427346, Avg test loss: 0.429232 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 103, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.312520, Avg test loss: 0.274243 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 104, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.164329, Avg test loss: 0.169814 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 104, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.175992, Avg test loss: 0.184141 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 104, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.164351, Avg test loss: 0.169844 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 104, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.436698, Avg test loss: 0.438627 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 104, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314491, Avg test loss: 0.274025 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 104, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.159017, Avg test loss: 0.167906 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 104, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.271700, Avg test loss: 0.269582 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 104, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.158991, Avg test loss: 0.167879 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 104, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.428649, Avg test loss: 0.430536 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 104, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314469, Avg test loss: 0.275242 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 104, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151297, Avg test loss: 0.161385 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 104, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.269115, Avg test loss: 0.266969 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 104, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151172, Avg test loss: 0.161329 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 104, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.427134, Avg test loss: 0.429016 \n",
      "\n",
      "paramters: k = 4, epochs = 250, verbose_epoch = 104, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.312562, Avg test loss: 0.274098 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 90, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160724, Avg test loss: 0.168482 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paramters: k = 4, epochs = 300, verbose_epoch = 90, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.267971, Avg test loss: 0.269876 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 90, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160753, Avg test loss: 0.168614 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 90, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.429543, Avg test loss: 0.431442 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 90, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314500, Avg test loss: 0.274853 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 90, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151227, Avg test loss: 0.161126 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 90, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.267417, Avg test loss: 0.266906 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 90, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151269, Avg test loss: 0.161272 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 90, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.427219, Avg test loss: 0.429094 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 90, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.312433, Avg test loss: 0.273847 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 90, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.143738, Avg test loss: 0.154561 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 90, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.224297, Avg test loss: 0.263348 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 90, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.143744, Avg test loss: 0.154495 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 90, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.427116, Avg test loss: 0.428985 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 90, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.309726, Avg test loss: 0.271282 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 91, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160765, Avg test loss: 0.168606 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 91, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.268564, Avg test loss: 0.269935 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 91, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160655, Avg test loss: 0.168509 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 91, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.429420, Avg test loss: 0.431309 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 91, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314523, Avg test loss: 0.274901 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 91, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151152, Avg test loss: 0.161114 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 91, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.267606, Avg test loss: 0.266906 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 91, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151251, Avg test loss: 0.161217 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 91, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.427242, Avg test loss: 0.429134 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 91, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.312427, Avg test loss: 0.273820 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 91, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.143712, Avg test loss: 0.154570 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 91, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.224946, Avg test loss: 0.263321 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 91, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.143694, Avg test loss: 0.154441 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 91, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.427118, Avg test loss: 0.429015 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 91, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.309738, Avg test loss: 0.271339 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 92, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160731, Avg test loss: 0.168588 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 92, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.268356, Avg test loss: 0.269910 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 92, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160668, Avg test loss: 0.168358 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 92, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.429398, Avg test loss: 0.431300 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 92, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314491, Avg test loss: 0.274801 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 92, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151264, Avg test loss: 0.161263 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 92, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.267770, Avg test loss: 0.266840 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 92, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151241, Avg test loss: 0.161181 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 92, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.427097, Avg test loss: 0.428987 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 92, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.312419, Avg test loss: 0.273912 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 92, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.143662, Avg test loss: 0.154523 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 92, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.225117, Avg test loss: 0.263297 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 92, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.143731, Avg test loss: 0.154485 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 92, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.427173, Avg test loss: 0.429063 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 92, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.309683, Avg test loss: 0.271250 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 93, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160709, Avg test loss: 0.168576 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 93, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.268699, Avg test loss: 0.269883 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 93, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160661, Avg test loss: 0.168499 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 93, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.429459, Avg test loss: 0.431362 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 93, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314519, Avg test loss: 0.274893 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 93, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151221, Avg test loss: 0.161211 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 93, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.267628, Avg test loss: 0.266904 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 93, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151169, Avg test loss: 0.161168 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paramters: k = 4, epochs = 300, verbose_epoch = 93, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.427153, Avg test loss: 0.429042 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 93, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.312461, Avg test loss: 0.274042 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 93, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.143725, Avg test loss: 0.154542 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 93, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.222725, Avg test loss: 0.263323 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 93, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.143700, Avg test loss: 0.154484 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 93, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.426854, Avg test loss: 0.428730 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 93, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.309746, Avg test loss: 0.271420 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 94, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160643, Avg test loss: 0.168454 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 94, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.268366, Avg test loss: 0.269870 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 94, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160686, Avg test loss: 0.168486 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 94, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.429447, Avg test loss: 0.431335 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 94, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314487, Avg test loss: 0.274886 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 94, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151253, Avg test loss: 0.161329 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 94, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.268687, Avg test loss: 0.266843 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 94, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151254, Avg test loss: 0.161273 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 94, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.427317, Avg test loss: 0.429218 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 94, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.312444, Avg test loss: 0.274041 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 94, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.143673, Avg test loss: 0.154510 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 94, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.224636, Avg test loss: 0.263367 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 94, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.143733, Avg test loss: 0.154538 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 94, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.426913, Avg test loss: 0.428799 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 94, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.309789, Avg test loss: 0.271379 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 95, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160654, Avg test loss: 0.168457 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 95, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.268439, Avg test loss: 0.269909 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 95, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160689, Avg test loss: 0.168623 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 95, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.429542, Avg test loss: 0.431442 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 95, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314471, Avg test loss: 0.274848 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 95, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151246, Avg test loss: 0.161249 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 95, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.268137, Avg test loss: 0.266888 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 95, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151181, Avg test loss: 0.161152 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 95, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.427135, Avg test loss: 0.429027 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 95, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.312413, Avg test loss: 0.273850 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 95, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.143727, Avg test loss: 0.154474 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 95, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.223487, Avg test loss: 0.263355 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 95, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.143735, Avg test loss: 0.154498 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 95, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.427139, Avg test loss: 0.429029 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 95, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.309737, Avg test loss: 0.271331 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 96, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160669, Avg test loss: 0.168568 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 96, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.268328, Avg test loss: 0.269873 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 96, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160640, Avg test loss: 0.168415 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 96, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.429516, Avg test loss: 0.431411 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 96, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314509, Avg test loss: 0.274798 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 96, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151246, Avg test loss: 0.161282 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 96, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.267675, Avg test loss: 0.266853 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 96, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151242, Avg test loss: 0.161210 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 96, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.427102, Avg test loss: 0.428982 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 96, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.312400, Avg test loss: 0.273990 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 96, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.143708, Avg test loss: 0.154552 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 96, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.224205, Avg test loss: 0.263368 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 96, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.143695, Avg test loss: 0.154493 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 96, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.426754, Avg test loss: 0.428637 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 96, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.309701, Avg test loss: 0.271392 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paramters: k = 4, epochs = 300, verbose_epoch = 97, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160707, Avg test loss: 0.168677 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 97, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.268387, Avg test loss: 0.269815 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 97, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160713, Avg test loss: 0.168552 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 97, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.429518, Avg test loss: 0.431427 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 97, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314473, Avg test loss: 0.274840 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 97, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151228, Avg test loss: 0.161187 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 97, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.267908, Avg test loss: 0.266817 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 97, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151223, Avg test loss: 0.161278 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 97, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.427224, Avg test loss: 0.429118 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 97, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.312426, Avg test loss: 0.273924 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 97, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.143735, Avg test loss: 0.154557 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 97, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.223327, Avg test loss: 0.263404 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 97, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.143709, Avg test loss: 0.154579 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 97, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.426823, Avg test loss: 0.428690 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 97, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.309728, Avg test loss: 0.271335 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 98, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160760, Avg test loss: 0.168503 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 98, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.268372, Avg test loss: 0.269882 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 98, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160755, Avg test loss: 0.168520 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 98, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.429462, Avg test loss: 0.431361 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 98, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314445, Avg test loss: 0.274791 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 98, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151243, Avg test loss: 0.161209 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 98, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.267872, Avg test loss: 0.266936 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 98, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151238, Avg test loss: 0.161352 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 98, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.427146, Avg test loss: 0.429024 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 98, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.312480, Avg test loss: 0.273991 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 98, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.143724, Avg test loss: 0.154540 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 98, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.223220, Avg test loss: 0.263371 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 98, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.143736, Avg test loss: 0.154546 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 98, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.427169, Avg test loss: 0.429056 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 98, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.309788, Avg test loss: 0.271402 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 99, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160710, Avg test loss: 0.168496 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 99, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.268408, Avg test loss: 0.269894 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 99, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160653, Avg test loss: 0.168547 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 99, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.429449, Avg test loss: 0.431345 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 99, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314481, Avg test loss: 0.274786 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 99, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151193, Avg test loss: 0.161157 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 99, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.268237, Avg test loss: 0.266840 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 99, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151173, Avg test loss: 0.161134 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 99, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.427308, Avg test loss: 0.429192 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 99, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.312442, Avg test loss: 0.273950 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 99, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.143658, Avg test loss: 0.154461 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 99, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.203393, Avg test loss: 0.263296 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 99, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.143686, Avg test loss: 0.154494 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 99, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.426890, Avg test loss: 0.428789 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 99, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.309689, Avg test loss: 0.271163 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 100, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160703, Avg test loss: 0.168522 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 100, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.268420, Avg test loss: 0.269819 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 100, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160636, Avg test loss: 0.168481 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 100, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.429539, Avg test loss: 0.431428 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 100, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314475, Avg test loss: 0.274843 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 100, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151264, Avg test loss: 0.161106 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 100, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.268524, Avg test loss: 0.266860 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paramters: k = 4, epochs = 300, verbose_epoch = 100, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151195, Avg test loss: 0.161248 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 100, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.427193, Avg test loss: 0.429098 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 100, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.312437, Avg test loss: 0.273885 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 100, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.143685, Avg test loss: 0.154502 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 100, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.222278, Avg test loss: 0.263317 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 100, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.143706, Avg test loss: 0.154455 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 100, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.427168, Avg test loss: 0.429056 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 100, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.309714, Avg test loss: 0.271323 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 101, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160627, Avg test loss: 0.168680 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 101, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.268393, Avg test loss: 0.269850 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 101, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160580, Avg test loss: 0.168611 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 101, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.429503, Avg test loss: 0.431395 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 101, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314464, Avg test loss: 0.274811 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 101, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151232, Avg test loss: 0.161251 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 101, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.267040, Avg test loss: 0.266894 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 101, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151286, Avg test loss: 0.161204 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 101, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.427339, Avg test loss: 0.429230 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 101, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.312447, Avg test loss: 0.273995 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 101, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.143718, Avg test loss: 0.154510 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 101, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.203247, Avg test loss: 0.263372 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 101, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.143709, Avg test loss: 0.154580 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 101, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.427092, Avg test loss: 0.428975 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 101, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.309780, Avg test loss: 0.271321 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 102, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160595, Avg test loss: 0.168552 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 102, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.268144, Avg test loss: 0.269844 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 102, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160651, Avg test loss: 0.168519 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 102, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.429572, Avg test loss: 0.431465 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 102, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314429, Avg test loss: 0.274788 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 102, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151210, Avg test loss: 0.161226 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 102, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.267615, Avg test loss: 0.266918 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 102, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151220, Avg test loss: 0.161186 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 102, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.427191, Avg test loss: 0.429069 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 102, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.312442, Avg test loss: 0.273897 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 102, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.143694, Avg test loss: 0.154429 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 102, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.223267, Avg test loss: 0.263304 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 102, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.143726, Avg test loss: 0.154565 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 102, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.426933, Avg test loss: 0.428808 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 102, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.309728, Avg test loss: 0.271246 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 103, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160680, Avg test loss: 0.168483 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 103, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.268530, Avg test loss: 0.269945 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 103, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160694, Avg test loss: 0.168410 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 103, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.429440, Avg test loss: 0.431339 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 103, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314504, Avg test loss: 0.274817 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 103, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151278, Avg test loss: 0.161324 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 103, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.267737, Avg test loss: 0.266925 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 103, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151228, Avg test loss: 0.161174 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 103, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.427297, Avg test loss: 0.429199 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 103, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.312434, Avg test loss: 0.273871 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 103, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.143703, Avg test loss: 0.154486 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 103, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.224219, Avg test loss: 0.263380 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 103, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.143686, Avg test loss: 0.154566 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 103, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.426992, Avg test loss: 0.428877 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paramters: k = 4, epochs = 300, verbose_epoch = 103, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.309767, Avg test loss: 0.271377 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 104, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160738, Avg test loss: 0.168581 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 104, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.268388, Avg test loss: 0.269865 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 104, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.160731, Avg test loss: 0.168563 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 104, learning_rate = 3, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.429493, Avg test loss: 0.431390 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 104, learning_rate = 3, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.314484, Avg test loss: 0.274795 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 104, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151205, Avg test loss: 0.161192 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 104, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.267669, Avg test loss: 0.266864 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 104, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.151167, Avg test loss: 0.161277 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 104, learning_rate = 4, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.427258, Avg test loss: 0.429140 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 104, learning_rate = 4, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.312425, Avg test loss: 0.273818 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 104, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.143690, Avg test loss: 0.154619 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 104, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.222742, Avg test loss: 0.263320 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 104, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.143701, Avg test loss: 0.154528 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 104, learning_rate = 5, weight_decay = 15\n",
      "4-fold validation: Avg train loss: 0.427074, Avg test loss: 0.428961 \n",
      "\n",
      "paramters: k = 4, epochs = 300, verbose_epoch = 104, learning_rate = 5, weight_decay = 0\n",
      "4-fold validation: Avg train loss: 0.309705, Avg test loss: 0.271213 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 90, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.211867, Avg test loss: 0.215702 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 90, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.215869, Avg test loss: 0.219832 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 90, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.211907, Avg test loss: 0.215819 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 90, learning_rate = 3, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.503147, Avg test loss: 0.505096 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 90, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.220446, Avg test loss: 0.224389 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 90, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.170566, Avg test loss: 0.175000 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 90, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.176369, Avg test loss: 0.180810 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 90, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.170517, Avg test loss: 0.174839 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 90, learning_rate = 4, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.457798, Avg test loss: 0.459592 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 90, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.186036, Avg test loss: 0.190997 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 90, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.164661, Avg test loss: 0.170343 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 90, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.183200, Avg test loss: 0.196369 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 90, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.164713, Avg test loss: 0.170217 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 90, learning_rate = 5, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.442164, Avg test loss: 0.443899 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 90, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.319125, Avg test loss: 0.271202 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 91, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.211946, Avg test loss: 0.215762 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 91, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.215916, Avg test loss: 0.219873 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 91, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.211887, Avg test loss: 0.215723 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 91, learning_rate = 3, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.503203, Avg test loss: 0.505148 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 91, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.220390, Avg test loss: 0.224249 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 91, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.170492, Avg test loss: 0.174840 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 91, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.176387, Avg test loss: 0.180737 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 91, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.170527, Avg test loss: 0.174906 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 91, learning_rate = 4, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.457749, Avg test loss: 0.459535 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 91, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.185881, Avg test loss: 0.190173 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 91, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.164713, Avg test loss: 0.170295 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 91, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.183160, Avg test loss: 0.197005 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 91, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.164591, Avg test loss: 0.170591 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 91, learning_rate = 5, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.442344, Avg test loss: 0.444079 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 91, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.319144, Avg test loss: 0.271311 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 92, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.211859, Avg test loss: 0.215701 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 92, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.215888, Avg test loss: 0.219845 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 92, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.211859, Avg test loss: 0.215710 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 92, learning_rate = 3, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.503132, Avg test loss: 0.505076 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 92, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.220324, Avg test loss: 0.224201 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 92, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.170561, Avg test loss: 0.174831 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paramters: k = 5, epochs = 150, verbose_epoch = 92, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.176351, Avg test loss: 0.180731 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 92, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.170539, Avg test loss: 0.174922 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 92, learning_rate = 4, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.457877, Avg test loss: 0.459672 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 92, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.186096, Avg test loss: 0.191063 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 92, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.164777, Avg test loss: 0.170300 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 92, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.183334, Avg test loss: 0.195909 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 92, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.164742, Avg test loss: 0.170562 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 92, learning_rate = 5, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.442182, Avg test loss: 0.443915 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 92, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.319162, Avg test loss: 0.271281 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 93, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.211950, Avg test loss: 0.215758 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 93, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.215888, Avg test loss: 0.219830 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 93, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.211839, Avg test loss: 0.215707 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 93, learning_rate = 3, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.503156, Avg test loss: 0.505098 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 93, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.220500, Avg test loss: 0.224417 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 93, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.170523, Avg test loss: 0.174814 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 93, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.176324, Avg test loss: 0.180634 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 93, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.170460, Avg test loss: 0.174882 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 93, learning_rate = 4, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.457792, Avg test loss: 0.459591 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 93, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.185845, Avg test loss: 0.191270 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 93, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.164692, Avg test loss: 0.170232 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 93, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.183079, Avg test loss: 0.198514 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 93, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.164581, Avg test loss: 0.170252 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 93, learning_rate = 5, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.442234, Avg test loss: 0.443968 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 93, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.319167, Avg test loss: 0.271327 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 94, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.211876, Avg test loss: 0.215845 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 94, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.215918, Avg test loss: 0.219805 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 94, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.211904, Avg test loss: 0.215769 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 94, learning_rate = 3, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.503218, Avg test loss: 0.505166 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 94, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.220436, Avg test loss: 0.224315 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 94, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.170519, Avg test loss: 0.174770 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 94, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.176217, Avg test loss: 0.180614 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 94, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.170555, Avg test loss: 0.174954 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 94, learning_rate = 4, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.457813, Avg test loss: 0.459611 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 94, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.185992, Avg test loss: 0.190885 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 94, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.164769, Avg test loss: 0.170333 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 94, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.182965, Avg test loss: 0.198319 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 94, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.164665, Avg test loss: 0.170336 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 94, learning_rate = 5, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.442375, Avg test loss: 0.444111 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 94, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.319166, Avg test loss: 0.271340 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 95, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.211879, Avg test loss: 0.215789 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 95, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.215876, Avg test loss: 0.219865 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 95, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.211886, Avg test loss: 0.215781 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 95, learning_rate = 3, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.503206, Avg test loss: 0.505147 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 95, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.220366, Avg test loss: 0.224236 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 95, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.170547, Avg test loss: 0.174871 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 95, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.176341, Avg test loss: 0.180570 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 95, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.170566, Avg test loss: 0.174895 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 95, learning_rate = 4, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.457749, Avg test loss: 0.459541 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 95, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.185946, Avg test loss: 0.190623 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 95, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.164575, Avg test loss: 0.170143 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 95, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.183545, Avg test loss: 0.199818 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 95, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.164760, Avg test loss: 0.170403 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paramters: k = 5, epochs = 150, verbose_epoch = 95, learning_rate = 5, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.442171, Avg test loss: 0.443903 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 95, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.319193, Avg test loss: 0.271330 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 96, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.211854, Avg test loss: 0.215788 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 96, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.215838, Avg test loss: 0.219813 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 96, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.211938, Avg test loss: 0.215797 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 96, learning_rate = 3, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.503232, Avg test loss: 0.505164 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 96, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.220400, Avg test loss: 0.224317 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 96, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.170596, Avg test loss: 0.174865 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 96, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.176319, Avg test loss: 0.180545 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 96, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.170579, Avg test loss: 0.174910 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 96, learning_rate = 4, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.457764, Avg test loss: 0.459560 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 96, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.185818, Avg test loss: 0.190169 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 96, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.164746, Avg test loss: 0.170195 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 96, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.183377, Avg test loss: 0.202651 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 96, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.164645, Avg test loss: 0.170152 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 96, learning_rate = 5, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.442324, Avg test loss: 0.444060 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 96, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.319119, Avg test loss: 0.271270 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 97, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.211904, Avg test loss: 0.215817 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 97, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.215876, Avg test loss: 0.219758 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 97, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.211805, Avg test loss: 0.215647 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 97, learning_rate = 3, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.503228, Avg test loss: 0.505166 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 97, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.220397, Avg test loss: 0.224331 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 97, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.170549, Avg test loss: 0.174907 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 97, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.176276, Avg test loss: 0.180551 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 97, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.170569, Avg test loss: 0.174870 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 97, learning_rate = 4, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.457850, Avg test loss: 0.459644 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 97, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.185656, Avg test loss: 0.191295 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 97, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.164624, Avg test loss: 0.170114 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 97, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.183413, Avg test loss: 0.194284 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 97, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.164715, Avg test loss: 0.170411 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 97, learning_rate = 5, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.442282, Avg test loss: 0.444001 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 97, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.319172, Avg test loss: 0.271414 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 98, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.211855, Avg test loss: 0.215847 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 98, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.215911, Avg test loss: 0.219801 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 98, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.211789, Avg test loss: 0.215638 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 98, learning_rate = 3, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.503098, Avg test loss: 0.505043 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 98, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.220425, Avg test loss: 0.224280 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 98, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.170538, Avg test loss: 0.174993 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 98, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.176351, Avg test loss: 0.180874 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 98, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.170483, Avg test loss: 0.174772 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 98, learning_rate = 4, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.457780, Avg test loss: 0.459576 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 98, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.185714, Avg test loss: 0.190873 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 98, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.164685, Avg test loss: 0.170451 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 98, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.183741, Avg test loss: 0.196220 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 98, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.164702, Avg test loss: 0.170141 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 98, learning_rate = 5, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.442280, Avg test loss: 0.444004 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 98, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.319204, Avg test loss: 0.271334 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 99, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.211825, Avg test loss: 0.215713 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 99, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.215975, Avg test loss: 0.219880 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 99, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.211932, Avg test loss: 0.215893 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 99, learning_rate = 3, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.503216, Avg test loss: 0.505168 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 99, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.220312, Avg test loss: 0.224233 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paramters: k = 5, epochs = 150, verbose_epoch = 99, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.170540, Avg test loss: 0.174769 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 99, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.176254, Avg test loss: 0.180790 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 99, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.170484, Avg test loss: 0.174746 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 99, learning_rate = 4, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.457740, Avg test loss: 0.459538 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 99, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.186053, Avg test loss: 0.190718 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 99, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.164619, Avg test loss: 0.170265 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 99, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.183174, Avg test loss: 0.194307 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 99, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.164675, Avg test loss: 0.170209 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 99, learning_rate = 5, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.442385, Avg test loss: 0.444113 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 99, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.319188, Avg test loss: 0.271368 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 100, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.211868, Avg test loss: 0.215803 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 100, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.215878, Avg test loss: 0.219746 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 100, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.211894, Avg test loss: 0.215831 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 100, learning_rate = 3, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.503279, Avg test loss: 0.505230 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 100, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.220413, Avg test loss: 0.224326 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 100, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.170545, Avg test loss: 0.174969 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 100, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.176221, Avg test loss: 0.180643 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 100, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.170585, Avg test loss: 0.174999 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 100, learning_rate = 4, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.457771, Avg test loss: 0.459568 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 100, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.186106, Avg test loss: 0.190811 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 100, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.164735, Avg test loss: 0.170411 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 100, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.183222, Avg test loss: 0.200892 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 100, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.164701, Avg test loss: 0.170296 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 100, learning_rate = 5, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.442227, Avg test loss: 0.443952 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 100, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.319144, Avg test loss: 0.271346 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 101, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.211941, Avg test loss: 0.215850 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 101, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.215872, Avg test loss: 0.219762 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 101, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.211993, Avg test loss: 0.215910 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 101, learning_rate = 3, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.503179, Avg test loss: 0.505117 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 101, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.220353, Avg test loss: 0.224226 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 101, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.170516, Avg test loss: 0.174893 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 101, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.176257, Avg test loss: 0.180713 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 101, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.170569, Avg test loss: 0.174935 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 101, learning_rate = 4, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.457672, Avg test loss: 0.459475 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 101, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.186024, Avg test loss: 0.191148 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 101, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.164750, Avg test loss: 0.170395 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 101, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.183443, Avg test loss: 0.200086 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 101, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.164748, Avg test loss: 0.170348 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 101, learning_rate = 5, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.442149, Avg test loss: 0.443886 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 101, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.319150, Avg test loss: 0.271254 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 102, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.211829, Avg test loss: 0.215787 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 102, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.215928, Avg test loss: 0.219871 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 102, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.211851, Avg test loss: 0.215740 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 102, learning_rate = 3, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.503192, Avg test loss: 0.505140 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 102, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.220356, Avg test loss: 0.224260 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 102, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.170550, Avg test loss: 0.174940 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 102, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.176347, Avg test loss: 0.180639 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 102, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.170489, Avg test loss: 0.174890 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 102, learning_rate = 4, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.457768, Avg test loss: 0.459564 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 102, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.185707, Avg test loss: 0.190285 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 102, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.164617, Avg test loss: 0.170383 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 102, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.182360, Avg test loss: 0.197937 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paramters: k = 5, epochs = 150, verbose_epoch = 102, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.164663, Avg test loss: 0.170243 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 102, learning_rate = 5, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.442474, Avg test loss: 0.444196 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 102, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.319200, Avg test loss: 0.271417 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 103, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.211844, Avg test loss: 0.215682 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 103, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.215908, Avg test loss: 0.219736 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 103, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.211847, Avg test loss: 0.215821 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 103, learning_rate = 3, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.503169, Avg test loss: 0.505118 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 103, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.220430, Avg test loss: 0.224325 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 103, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.170506, Avg test loss: 0.174822 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 103, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.176254, Avg test loss: 0.180685 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 103, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.170527, Avg test loss: 0.174887 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 103, learning_rate = 4, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.457728, Avg test loss: 0.459520 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 103, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.186039, Avg test loss: 0.191617 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 103, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.164661, Avg test loss: 0.170486 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 103, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.182598, Avg test loss: 0.203676 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 103, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.164646, Avg test loss: 0.170156 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 103, learning_rate = 5, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.442372, Avg test loss: 0.444097 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 103, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.319165, Avg test loss: 0.271305 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 104, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.211874, Avg test loss: 0.215774 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 104, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.215892, Avg test loss: 0.219735 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 104, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.211799, Avg test loss: 0.215632 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 104, learning_rate = 3, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.503207, Avg test loss: 0.505152 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 104, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.220444, Avg test loss: 0.224374 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 104, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.170548, Avg test loss: 0.174961 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 104, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.176273, Avg test loss: 0.180706 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 104, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.170495, Avg test loss: 0.174957 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 104, learning_rate = 4, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.457711, Avg test loss: 0.459503 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 104, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.185681, Avg test loss: 0.190599 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 104, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.164619, Avg test loss: 0.170183 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 104, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.183741, Avg test loss: 0.192864 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 104, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.164707, Avg test loss: 0.170232 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 104, learning_rate = 5, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.442274, Avg test loss: 0.443999 \n",
      "\n",
      "paramters: k = 5, epochs = 150, verbose_epoch = 104, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.319186, Avg test loss: 0.271279 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 90, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.169055, Avg test loss: 0.173419 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 90, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.175102, Avg test loss: 0.179505 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 90, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.169043, Avg test loss: 0.173482 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 90, learning_rate = 3, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.454493, Avg test loss: 0.456275 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 90, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.185781, Avg test loss: 0.191790 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 90, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.163232, Avg test loss: 0.169792 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 90, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.252960, Avg test loss: 0.266189 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 90, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.163237, Avg test loss: 0.169615 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 90, learning_rate = 4, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.438527, Avg test loss: 0.440243 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 90, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.319520, Avg test loss: 0.272161 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 90, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.157835, Avg test loss: 0.166236 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 90, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.313931, Avg test loss: 0.265656 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 90, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.157877, Avg test loss: 0.166401 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 90, learning_rate = 5, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.434660, Avg test loss: 0.436349 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 90, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.319761, Avg test loss: 0.273329 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 91, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.168981, Avg test loss: 0.173481 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 91, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.175052, Avg test loss: 0.179492 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 91, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.169060, Avg test loss: 0.173458 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 91, learning_rate = 3, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.454476, Avg test loss: 0.456258 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paramters: k = 5, epochs = 200, verbose_epoch = 91, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.186084, Avg test loss: 0.191084 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 91, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.163308, Avg test loss: 0.169776 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 91, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.252245, Avg test loss: 0.266189 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 91, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.163242, Avg test loss: 0.169611 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 91, learning_rate = 4, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.438460, Avg test loss: 0.440173 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 91, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.319544, Avg test loss: 0.272105 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 91, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.157848, Avg test loss: 0.166296 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 91, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.313943, Avg test loss: 0.265717 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 91, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.157928, Avg test loss: 0.166476 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 91, learning_rate = 5, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.434863, Avg test loss: 0.436550 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 91, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.319844, Avg test loss: 0.273287 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 92, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.169085, Avg test loss: 0.173447 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 92, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.175072, Avg test loss: 0.179622 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 92, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.169047, Avg test loss: 0.173460 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 92, learning_rate = 3, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.454519, Avg test loss: 0.456303 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 92, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.186033, Avg test loss: 0.191239 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 92, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.163280, Avg test loss: 0.169573 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 92, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.253074, Avg test loss: 0.266192 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 92, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.163264, Avg test loss: 0.169689 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 92, learning_rate = 4, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.438606, Avg test loss: 0.440322 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 92, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.319456, Avg test loss: 0.272176 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 92, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.157849, Avg test loss: 0.166521 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 92, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.313929, Avg test loss: 0.265632 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 92, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.157856, Avg test loss: 0.166440 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 92, learning_rate = 5, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.434921, Avg test loss: 0.436623 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 92, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.319720, Avg test loss: 0.273418 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 93, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.168998, Avg test loss: 0.173417 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 93, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.175081, Avg test loss: 0.179485 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 93, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.168999, Avg test loss: 0.173459 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 93, learning_rate = 3, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.454496, Avg test loss: 0.456275 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 93, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.185841, Avg test loss: 0.191338 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 93, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.163307, Avg test loss: 0.169586 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 93, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.253476, Avg test loss: 0.266174 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 93, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.163237, Avg test loss: 0.169534 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 93, learning_rate = 4, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.438490, Avg test loss: 0.440211 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 93, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.319519, Avg test loss: 0.272176 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 93, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.157886, Avg test loss: 0.166285 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 93, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.313919, Avg test loss: 0.265707 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 93, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.157799, Avg test loss: 0.166394 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 93, learning_rate = 5, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.434831, Avg test loss: 0.436529 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 93, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.319827, Avg test loss: 0.273418 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 94, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.169128, Avg test loss: 0.173484 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 94, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.175124, Avg test loss: 0.179443 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 94, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.169019, Avg test loss: 0.173454 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 94, learning_rate = 3, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.454499, Avg test loss: 0.456282 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 94, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.186249, Avg test loss: 0.191350 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 94, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.163180, Avg test loss: 0.169485 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 94, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.252494, Avg test loss: 0.266135 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 94, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.163213, Avg test loss: 0.169501 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 94, learning_rate = 4, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.438442, Avg test loss: 0.440163 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 94, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.319527, Avg test loss: 0.272116 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 94, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.157907, Avg test loss: 0.166217 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paramters: k = 5, epochs = 200, verbose_epoch = 94, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.313889, Avg test loss: 0.265669 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 94, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.157834, Avg test loss: 0.166202 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 94, learning_rate = 5, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.434946, Avg test loss: 0.436637 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 94, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.319797, Avg test loss: 0.273329 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 95, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.169062, Avg test loss: 0.173485 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 95, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.175111, Avg test loss: 0.179562 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 95, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.168970, Avg test loss: 0.173371 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 95, learning_rate = 3, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.454471, Avg test loss: 0.456263 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 95, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.185930, Avg test loss: 0.191493 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 95, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.163248, Avg test loss: 0.169781 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 95, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.253242, Avg test loss: 0.266179 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 95, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.163228, Avg test loss: 0.169614 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 95, learning_rate = 4, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.438536, Avg test loss: 0.440256 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 95, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.319533, Avg test loss: 0.272101 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 95, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.157880, Avg test loss: 0.166406 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 95, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.313910, Avg test loss: 0.265666 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 95, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.157925, Avg test loss: 0.166225 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 95, learning_rate = 5, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.434801, Avg test loss: 0.436498 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 95, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.319800, Avg test loss: 0.273274 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 96, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.169024, Avg test loss: 0.173349 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 96, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.175017, Avg test loss: 0.179604 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 96, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.169089, Avg test loss: 0.173560 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 96, learning_rate = 3, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.454525, Avg test loss: 0.456315 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 96, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.186142, Avg test loss: 0.191364 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 96, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.163355, Avg test loss: 0.169857 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 96, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.252475, Avg test loss: 0.266153 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 96, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.163248, Avg test loss: 0.169496 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 96, learning_rate = 4, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.438559, Avg test loss: 0.440279 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 96, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.319506, Avg test loss: 0.272134 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 96, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.157843, Avg test loss: 0.166252 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 96, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.313851, Avg test loss: 0.265658 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 96, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.157861, Avg test loss: 0.166113 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 96, learning_rate = 5, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.434877, Avg test loss: 0.436576 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 96, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.319853, Avg test loss: 0.273423 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 97, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.169056, Avg test loss: 0.173338 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 97, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.175197, Avg test loss: 0.179551 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 97, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.169087, Avg test loss: 0.173488 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 97, learning_rate = 3, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.454452, Avg test loss: 0.456232 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 97, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.186086, Avg test loss: 0.191359 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 97, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.163219, Avg test loss: 0.169769 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 97, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.252568, Avg test loss: 0.266091 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 97, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.163277, Avg test loss: 0.169706 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 97, learning_rate = 4, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.438474, Avg test loss: 0.440184 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 97, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.319510, Avg test loss: 0.272157 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 97, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.157856, Avg test loss: 0.166372 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 97, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.313865, Avg test loss: 0.265652 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 97, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.157888, Avg test loss: 0.166253 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 97, learning_rate = 5, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.434855, Avg test loss: 0.436558 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 97, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.319842, Avg test loss: 0.273281 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 98, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.169040, Avg test loss: 0.173482 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 98, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.175098, Avg test loss: 0.179675 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 98, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.169071, Avg test loss: 0.173447 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paramters: k = 5, epochs = 200, verbose_epoch = 98, learning_rate = 3, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.454490, Avg test loss: 0.456271 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 98, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.185725, Avg test loss: 0.191826 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 98, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.163214, Avg test loss: 0.169613 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 98, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.253392, Avg test loss: 0.266156 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 98, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.163294, Avg test loss: 0.169511 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 98, learning_rate = 4, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.438574, Avg test loss: 0.440286 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 98, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.319546, Avg test loss: 0.272197 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 98, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.157873, Avg test loss: 0.166248 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 98, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.313859, Avg test loss: 0.265710 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 98, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.157775, Avg test loss: 0.166377 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 98, learning_rate = 5, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.434878, Avg test loss: 0.436581 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 98, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.319743, Avg test loss: 0.273487 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 99, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.169067, Avg test loss: 0.173531 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 99, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.175180, Avg test loss: 0.179593 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 99, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.169090, Avg test loss: 0.173590 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 99, learning_rate = 3, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.454525, Avg test loss: 0.456316 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 99, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.186018, Avg test loss: 0.191313 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 99, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.163271, Avg test loss: 0.169648 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 99, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.252871, Avg test loss: 0.266216 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 99, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.163138, Avg test loss: 0.169461 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 99, learning_rate = 4, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.438519, Avg test loss: 0.440245 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 99, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.319498, Avg test loss: 0.272061 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 99, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.157929, Avg test loss: 0.166367 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 99, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.313928, Avg test loss: 0.265721 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 99, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.157959, Avg test loss: 0.166489 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 99, learning_rate = 5, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.434851, Avg test loss: 0.436549 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 99, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.319878, Avg test loss: 0.273529 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 100, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.169028, Avg test loss: 0.173413 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 100, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.175159, Avg test loss: 0.179693 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 100, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.169040, Avg test loss: 0.173454 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 100, learning_rate = 3, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.454479, Avg test loss: 0.456263 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 100, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.186016, Avg test loss: 0.191300 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 100, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.163330, Avg test loss: 0.169796 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 100, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.253224, Avg test loss: 0.266212 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 100, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.163192, Avg test loss: 0.169813 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 100, learning_rate = 4, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.438513, Avg test loss: 0.440218 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 100, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.319459, Avg test loss: 0.272150 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 100, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.157956, Avg test loss: 0.166443 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 100, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.313881, Avg test loss: 0.265618 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 100, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.157867, Avg test loss: 0.166250 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 100, learning_rate = 5, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.434893, Avg test loss: 0.436593 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 100, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.319768, Avg test loss: 0.273351 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 101, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.169002, Avg test loss: 0.173448 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 101, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.175061, Avg test loss: 0.179403 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 101, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.169108, Avg test loss: 0.173490 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 101, learning_rate = 3, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.454431, Avg test loss: 0.456222 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 101, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.185677, Avg test loss: 0.191909 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 101, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.163278, Avg test loss: 0.169715 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 101, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.252534, Avg test loss: 0.266123 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 101, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.163341, Avg test loss: 0.169524 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 101, learning_rate = 4, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.438536, Avg test loss: 0.440249 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 101, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.319520, Avg test loss: 0.272136 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paramters: k = 5, epochs = 200, verbose_epoch = 101, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.157885, Avg test loss: 0.166301 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 101, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.313929, Avg test loss: 0.265615 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 101, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.157838, Avg test loss: 0.166387 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 101, learning_rate = 5, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.434730, Avg test loss: 0.436428 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 101, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.319846, Avg test loss: 0.273341 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 102, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.169003, Avg test loss: 0.173506 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 102, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.175058, Avg test loss: 0.179477 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 102, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.169148, Avg test loss: 0.173466 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 102, learning_rate = 3, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.454441, Avg test loss: 0.456230 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 102, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.186059, Avg test loss: 0.191261 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 102, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.163341, Avg test loss: 0.169730 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 102, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.251843, Avg test loss: 0.266134 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 102, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.163239, Avg test loss: 0.169797 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 102, learning_rate = 4, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.438407, Avg test loss: 0.440123 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 102, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.319542, Avg test loss: 0.272082 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 102, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.157870, Avg test loss: 0.166349 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 102, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.313914, Avg test loss: 0.265677 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 102, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.157845, Avg test loss: 0.166197 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 102, learning_rate = 5, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.434798, Avg test loss: 0.436500 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 102, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.319837, Avg test loss: 0.273322 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 103, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.169068, Avg test loss: 0.173403 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 103, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.175113, Avg test loss: 0.179531 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 103, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.169072, Avg test loss: 0.173471 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 103, learning_rate = 3, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.454410, Avg test loss: 0.456199 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 103, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.185931, Avg test loss: 0.192325 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 103, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.163204, Avg test loss: 0.169608 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 103, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.252567, Avg test loss: 0.266236 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 103, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.163366, Avg test loss: 0.169854 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 103, learning_rate = 4, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.438504, Avg test loss: 0.440213 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 103, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.319476, Avg test loss: 0.272257 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 103, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.157885, Avg test loss: 0.166241 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 103, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.313902, Avg test loss: 0.265688 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 103, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.157834, Avg test loss: 0.165998 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 103, learning_rate = 5, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.434783, Avg test loss: 0.436474 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 103, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.319806, Avg test loss: 0.273393 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 104, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.169098, Avg test loss: 0.173434 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 104, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.175156, Avg test loss: 0.179676 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 104, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.169034, Avg test loss: 0.173479 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 104, learning_rate = 3, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.454532, Avg test loss: 0.456319 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 104, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.185964, Avg test loss: 0.191714 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 104, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.163207, Avg test loss: 0.169504 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 104, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.253098, Avg test loss: 0.266210 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 104, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.163194, Avg test loss: 0.169339 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 104, learning_rate = 4, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.438588, Avg test loss: 0.440304 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 104, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.319501, Avg test loss: 0.272065 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 104, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.157939, Avg test loss: 0.166228 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 104, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.313938, Avg test loss: 0.265698 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 104, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.157910, Avg test loss: 0.166237 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 104, learning_rate = 5, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.434879, Avg test loss: 0.436576 \n",
      "\n",
      "paramters: k = 5, epochs = 200, verbose_epoch = 104, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.319847, Avg test loss: 0.273319 \n",
      "\n",
      "paramters: k = 5, epochs = 250, verbose_epoch = 90, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.163411, Avg test loss: 0.169548 \n",
      "\n",
      "paramters: k = 5, epochs = 250, verbose_epoch = 90, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.192273, Avg test loss: 0.265920 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paramters: k = 5, epochs = 250, verbose_epoch = 90, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.163456, Avg test loss: 0.169466 \n",
      "\n",
      "paramters: k = 5, epochs = 250, verbose_epoch = 90, learning_rate = 3, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.439242, Avg test loss: 0.440958 \n",
      "\n",
      "paramters: k = 5, epochs = 250, verbose_epoch = 90, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.319190, Avg test loss: 0.271619 \n",
      "\n",
      "paramters: k = 5, epochs = 250, verbose_epoch = 90, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.156248, Avg test loss: 0.164916 \n",
      "\n",
      "paramters: k = 5, epochs = 250, verbose_epoch = 90, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.313363, Avg test loss: 0.264990 \n",
      "\n",
      "paramters: k = 5, epochs = 250, verbose_epoch = 90, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.156291, Avg test loss: 0.164824 \n",
      "\n",
      "paramters: k = 5, epochs = 250, verbose_epoch = 90, learning_rate = 4, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.434517, Avg test loss: 0.436213 \n",
      "\n",
      "paramters: k = 5, epochs = 250, verbose_epoch = 90, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.319337, Avg test loss: 0.272856 \n",
      "\n",
      "paramters: k = 5, epochs = 250, verbose_epoch = 90, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.147948, Avg test loss: 0.158259 \n",
      "\n",
      "paramters: k = 5, epochs = 250, verbose_epoch = 90, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.310419, Avg test loss: 0.261733 \n",
      "\n",
      "paramters: k = 5, epochs = 250, verbose_epoch = 90, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.147932, Avg test loss: 0.158059 \n",
      "\n",
      "paramters: k = 5, epochs = 250, verbose_epoch = 90, learning_rate = 5, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.433791, Avg test loss: 0.435475 \n",
      "\n",
      "paramters: k = 5, epochs = 250, verbose_epoch = 90, learning_rate = 5, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.317306, Avg test loss: 0.270470 \n",
      "\n",
      "paramters: k = 5, epochs = 250, verbose_epoch = 91, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.163481, Avg test loss: 0.169463 \n",
      "\n",
      "paramters: k = 5, epochs = 250, verbose_epoch = 91, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.195329, Avg test loss: 0.265880 \n",
      "\n",
      "paramters: k = 5, epochs = 250, verbose_epoch = 91, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.163472, Avg test loss: 0.169400 \n",
      "\n",
      "paramters: k = 5, epochs = 250, verbose_epoch = 91, learning_rate = 3, weight_decay = 15\n",
      "5-fold validation: Avg train loss: 0.439075, Avg test loss: 0.440794 \n",
      "\n",
      "paramters: k = 5, epochs = 250, verbose_epoch = 91, learning_rate = 3, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.319197, Avg test loss: 0.271548 \n",
      "\n",
      "paramters: k = 5, epochs = 250, verbose_epoch = 91, learning_rate = 4, weight_decay = 0\n",
      "5-fold validation: Avg train loss: 0.156316, Avg test loss: 0.164938 \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-cf6d5a631263>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mweight_decay\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                     train_loss, test_loss = k_fold_cross_valid(k, epochs, verbose_epoch, X_train,\n\u001b[0;32m---> 10\u001b[0;31m                                                                y_train, learning_rate, weight_decay)\n\u001b[0m\u001b[1;32m     11\u001b[0m                     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'k'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'verbose_epoch'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mverbose_epoch\u001b[0m\u001b[0;34m,\u001b[0m                                  \u001b[0;34m'learning_rate'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'score'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-d08089130cf5>\u001b[0m in \u001b[0;36mk_fold_cross_valid\u001b[0;34m(k, epochs, verbose_epoch, X_train, y_train, learning_rate, weight_decay)\u001b[0m\n\u001b[1;32m     24\u001b[0m         train_loss, test_loss = train(\n\u001b[1;32m     25\u001b[0m             \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             epochs, verbose_epoch, learning_rate, weight_decay)\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mtrain_loss_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m#print(\"Test loss: %f\" % test_loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-1de518caf7b2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, X_train, y_train, X_test, y_test, epochs, verbose_epoch, learning_rate, weight_decay)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce_reinit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_iter_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/gluon/lib/python3.6/site-packages/mxnet/gluon/data/dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_sampler\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0m_batchify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/gluon/lib/python3.6/site-packages/mxnet/gluon/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_sampler\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0m_batchify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/gluon/lib/python3.6/site-packages/mxnet/gluon/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/gluon/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;31m# multi-dimensional slicing is not supported yet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteger_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m                 raise IndexError(\n\u001b[1;32m    506\u001b[0m                     'index {} is out of bounds for axis 0 with size {}'.format(\n",
      "\u001b[0;32m/anaconda/envs/gluon/lib/python3.6/site-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36mshape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1335\u001b[0m         \"\"\"\n\u001b[1;32m   1336\u001b[0m         \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmx_uint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1337\u001b[0;31m         \u001b[0mpdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPOINTER\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmx_uint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1338\u001b[0m         check_call(_LIB.MXNDArrayGetShape(\n\u001b[1;32m   1339\u001b[0m             self.handle, ctypes.byref(ndim), ctypes.byref(pdata)))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl8AAAGjCAYAAAASB3DiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11c8c1128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = pd.DataFrame()\n",
    "best_para = []\n",
    "best_score = 1.0\n",
    "for k in range(4,10):\n",
    "    for epochs in [150,200,250,300]:\n",
    "        for verbose_epoch in range(90,105):\n",
    "            for learning_rate in range(3,6):\n",
    "                for weight_decay in [0.0,0.1,0,15,0.2]:\n",
    "                    train_loss, test_loss = k_fold_cross_valid(k, epochs, verbose_epoch, X_train,\n",
    "                                                               y_train, learning_rate, weight_decay)\n",
    "                    score = (train_loss + test_loss) / 2.0\n",
    "                    result.append([{'k':k,'epochs':epochs,'verbose_epoch':verbose_epoch,\\\n",
    "                                  'learning_rate':learning_rate, 'weight_decay': weight_decay, 'score': score}])\n",
    "                    if score < best_score:\n",
    "                        best_score=score\n",
    "                        best_para = [k, epochs, verbose_epoch, learning_rate, weight_decay]\n",
    "                    print(\"paramters: k = %d, epochs = %d, verbose_epoch = %d, learning_rate = %d, weight_decay = %d\"\n",
    "                         %(k,epochs,verbose_epoch,learning_rate,weight_decay))\n",
    "                    print(\"%d-fold validation: Avg train loss: %f, Avg test loss: %f \\n\" %(k, train_loss, test_loss))\n",
    "\n",
    "print(\"best score = %d\" %(best_score))\n",
    "print(\", with paramters:\", best_para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Kaggle submission\n",
    "\n",
    "def learn(epochs, verbose_epoch, X_train, y_train, test, learning_rate,\n",
    "          weight_decay):\n",
    "    net = get_net()\n",
    "    train(net, X_train, y_train, None, None, epochs, verbose_epoch,\n",
    "          learning_rate, weight_decay)\n",
    "    preds = net(X_test).asnumpy()\n",
    "    test['SalePrice'] = pd.Series(preds.reshape(1, -1)[0])\n",
    "    submission = pd.concat([test['Id'], test['SalePrice']], axis=1)\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "    \n",
    "#learn(epochs, verbose_epoch, X_train, y_train, test, learning_rate, weight_decay)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
